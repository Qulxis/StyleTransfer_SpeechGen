{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode/Tokenize\n",
    "1. In order to feed the model tweets and have them treated \"independently\" (in reality several are read in a single batch), we separate tweets using the special token \"<|endoftext|>\",  used in pre-training by Open AI to separate documents. Our dataset then becomes something like:\n",
    "<|endoftext|>This is my first tweet!<|endoftext|>Second tweet already!<|endoftext|>\n",
    "\n",
    "Note: having no space around <|endoftext|> empirically leads to better predictions.\n",
    "\n",
    "2. Mix tweets and shuffling for each epoch and then reapply 1.\n",
    "3. 80% --> training, 20% --> validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune Model:\n",
    "1. Import GPT-2 small from: https://huggingface.co/docs/transformers/index\n",
    "2. Fine tune model (take a look at run_language_modeling.py and run_generation.py)\n",
    "3. Consider using sweets to get optimal hyper parameters\n",
    "4. Consider using RF on input parameters vs val loss to generate features importance table\n",
    "\n",
    "Their best choices were:\n",
    "    cosine learning scheduler\n",
    "    no gradient accumulation\n",
    "    no warmup\n",
    "    4 epochs\n",
    "    learning rate of 1.37e-4\n",
    "\n",
    "Things to consider:\n",
    "learning rate scheduler, number of epochs, learning rate, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## NOTE:\n",
    "Code block below are a draft. A final working version with wandb is in the colab notebook linked below.\n",
    "To run, you will need to upload the .txt data file for the user that is Generated in \"Scrapping and Cleaning.ipynb\"\n",
    "\n",
    "https://colab.research.google.com/drive/19iV11ZU4mX9JeZfVchC21uROXRugYkw4?authuser=1#scrollTo=ZzPjnbH8YXdp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft: \n",
    "### Do not run below as will lag out computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import (\n",
    "            AutoTokenizer, AutoModelForCausalLM,\n",
    "            TextDataset, DataCollatorForLanguageModeling,\n",
    "            Trainer, TrainingArguments,\n",
    "            get_cosine_schedule_with_warmup)\n",
    "# from huggingface_hub.hf_api import HfAp\n",
    "import api.filter\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import pathlib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:54: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (913001 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#Tokenize:\n",
    "file_path = \"./data/cleaned_SenSanders_5000.txt\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "block_size = tokenizer.model_max_length\n",
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=file_path, block_size=block_size, overwrite_cache=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train:\n",
    "#Largely taken from: https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb#scrollTo=ZSCf6QyF8AG-\n",
    "\n",
    "ALLOW_NEW_LINES = False     # seems to work better <--- from source\n",
    "LEARNING_RATE = 1.372e-4\n",
    "EPOCHS = 4\n",
    "seed = random.randint(0,2**32-1)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model_files\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=5,\n",
    "    save_steps=0,\n",
    "    seed=seed,\n",
    "    learning_rate = LEARNING_RATE)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.model.config.task_specific_params['text-generation'] = {\n",
    "    'do_sample': True,\n",
    "    'min_length': 10,\n",
    "    'max_length': 160,\n",
    "    'temperature': 1.,\n",
    "    'top_p': 0.95,\n",
    "    'prefix': '<|endoftext|>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model:\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction step\n",
    "start_with_bos = '<|endoftext|>' + start\n",
    "encoded_prompt = trainer.tokenizer(start_with_bos, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "encoded_prompt = encoded_prompt.to(trainer.model.device)\n",
    "\n",
    "# prediction\n",
    "output_sequences = trainer.model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    max_length=160,\n",
    "    min_length=10,\n",
    "    temperature=1.,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=10\n",
    "    )\n",
    "generated_sequences = []\n",
    "\n",
    "# decode prediction\n",
    "predictions = []\n",
    "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    text = trainer.tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
    "    if not ALLOW_NEW_LINES:\n",
    "        limit = text.find('\\n')\n",
    "        text = text[: limit if limit != -1 else None]\n",
    "    generated_sequences.append(text.strip())\n",
    "\n",
    "for i, g in enumerate(generated_sequences):\n",
    "    predictions.append([start, g])a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "08e595c52ca3b9470036b1110e67b559e55f367cabc363f2e28d35631ed95060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
