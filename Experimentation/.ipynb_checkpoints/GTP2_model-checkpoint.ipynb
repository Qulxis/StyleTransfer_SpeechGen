{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTsPagZoU-bL",
    "outputId": "ec7d7bd6-6d34-4873-ad48-bd04766a345b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-tji2ivgr\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-tji2ivgr\n",
      "  Resolved https://github.com/huggingface/transformers to commit 799cea64ac1029d66e9e58f18bc6f47892270723\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from transformers==4.26.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from transformers==4.26.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from transformers==4.26.0.dev0) (21.0)\n",
      "Requirement already satisfied: requests in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from transformers==4.26.0.dev0) (2.26.0)\n",
      "Requirement already satisfied: filelock in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from transformers==4.26.0.dev0) (3.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from transformers==4.26.0.dev0) (2022.10.31)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from transformers==4.26.0.dev0) (6.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement huggingface-hub<1.0,>=0.10.0 (from transformers) (from versions: 0.0.1, 0.0.2, 0.0.3rc1, 0.0.3rc2, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.9, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.0.14, 0.0.15, 0.0.16, 0.0.17, 0.0.18, 0.0.19, 0.1.0, 0.1.1, 0.1.2, 0.2.0, 0.2.1, 0.4.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for huggingface-hub<1.0,>=0.10.0\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ecbm4040/envTF24/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: wandb in /home/ecbm4040/envTF24/lib/python3.6/site-packages (0.13.6)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (1.11.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: setuptools in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (58.0.2)\n",
      "Requirement already satisfied: dataclasses in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (0.8)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (3.1.18)\n",
      "Requirement already satisfied: pathtools in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: setproctitle in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (3.17.3)\n",
      "Requirement already satisfied: PyYAML in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: importlib-metadata in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.8.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.5.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ecbm4040/envTF24/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "id": "EHvMfwd3iz3i"
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "PmcAtJ61ToFn",
    "outputId": "a98a7842-d346-418e-8fe6-7b2edf3adb75"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href=\"https://wandb.me/wandb-init\" target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ecbm4040/StyleTransfer_SpeechGen/Experimentation/wandb/run-20221212_011403-2q8nhdyb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/qulxis/my-awesome-project/runs/2q8nhdyb\" target=\"_blank\">rich-water-21</a></strong> to <a href=\"https://wandb.ai/qulxis/my-awesome-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import (\n",
    "            AutoTokenizer, AutoModelForCausalLM,\n",
    "            TextDataset, DataCollatorForLanguageModeling,\n",
    "            Trainer, TrainingArguments,\n",
    "            get_cosine_schedule_with_warmup)\n",
    "# from huggingface_hub.hf_api import HfAp\n",
    "# import api.filter\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"my-awesome-project\")\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlnX7c1OUl0v",
    "outputId": "0227d49b-3cb0-403c-f21c-79c0e219677d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/ecbm4040/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /home/ecbm4040/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /home/ecbm4040/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /home/ecbm4040/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/ecbm4040/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/ecbm4040/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /home/ecbm4040/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "/home/ecbm4040/envTF24/lib/python3.6/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "Creating features from dataset file at data\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (443257 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Saving features into cached file data/cached_lm_GPT2TokenizerFast_1024_polite_hankgreen_50000_0.5_finalnum3541.txt [took 0.009 s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=tweet_analysis\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize:\n",
    "file_path = \"data/polite_hankgreen_50000_0.5_finalnum3541.txt\"\n",
    "wandb.run.name = file_path\n",
    "\"\"\"\n",
    "Polite Options:\n",
    "polite_hankgreen_50000_0.5_finalnum3541.txt\n",
    "\n",
    "Impolite Options:\n",
    "impolite_hankgreen_50000_0.25_finalnum7746.txt\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "block_size = tokenizer.model_max_length\n",
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=file_path, block_size=block_size, overwrite_cache=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "#setup wanbi:\n",
    "%env WANDB_PROJECT=tweet_analysis\n",
    "\n",
    "wandb.run.name = file_path\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N75aQ7ucUoDD",
    "outputId": "eebc6dbf-8d38-425c-fdf2-3125b7aba852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 12 01:15:57 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   76C    P0    33W /  70W |  14068MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      3979      C   ...m4040/envTF24/bin/python3    14065MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6tkauYiUpct",
    "outputId": "b13ea060-cf9f-49b3-d990-b1228971f57a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "/home/ecbm4040/envTF24/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "#Train:\n",
    "#Largely taken from: https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb#scrollTo=ZSCf6QyF8AG-\n",
    "\n",
    "ALLOW_NEW_LINES = False     # seems to work better <--- from source\n",
    "LEARNING_RATE = 1.372e-4\n",
    "EPOCHS = 10\n",
    "seed = random.randint(0,2**32-1)\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./model_files\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=5,\n",
    "    save_steps=0,\n",
    "    seed=seed,\n",
    "    learning_rate = LEARNING_RATE\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset)\n",
    "#LR schedule stuff?\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "num_train_steps = len(train_dataloader)\n",
    "trainer.create_optimizer_and_scheduler(num_train_steps)\n",
    "trainer.lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    trainer.optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_train_steps)\n",
    "\n",
    "# p_start, p_end = 0.4, 0.9\n",
    "# def progressify(f):\n",
    "#     \"Control progress bar when calling f\"\n",
    "#     def inner(*args, **kwargs):\n",
    "#         if trainer.state.epoch is not None:\n",
    "#             # we only have one epoch, EPOCHS is built into dataset\n",
    "#             progress.value = p_start + trainer.state.epoch * (p_end - p_start)\n",
    "#         return f(*args, **kwargs)\n",
    "#     return inner\n",
    "\n",
    "# trainer.training_step = progressify(trainer.training_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "753de470647f4d58ab2454117f2610b3",
      "8f2e1d44a4bd491f91db7a44a5fa0938",
      "02e007fb503c4abea13fcdab99ba672c",
      "9c1cdb3f8f504888ae1947271af1061f",
      "04f58430b59b4df8b5352bda202b47f9",
      "7fe9e859d70e4399bf8e61f1382ccdd3",
      "cfb9c954923747d586a85fc1cf1442dc",
      "98e279404402473783c5dafc59e78b58"
     ]
    },
    "id": "Y2RvzAyfUrNn",
    "outputId": "08be6368-6aa1-4565-d940-51a7f0fdae70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 432\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 432\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 03:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.931100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.984700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.904400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.965600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.867600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.866300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.890700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.863600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.893900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.839900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.775500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>3.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.695900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>3.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>3.591900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.690100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>3.586800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.660100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>3.598500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.504100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>3.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>3.495200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>3.565300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>3.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>3.483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>3.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.457300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>3.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>3.335700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.335300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>3.387700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.401500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>3.402200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.374300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>3.329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>3.278400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>3.277400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>3.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>3.380500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.245700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>3.233500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>3.234800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>3.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>3.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>3.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>3.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>3.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>3.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>3.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>3.194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>3.223700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>3.179200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>3.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>3.160400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>3.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>3.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>3.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>3.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>3.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>3.278700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>3.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>3.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>3.071800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>3.173000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>3.147600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>3.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>3.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>3.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>3.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>3.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>3.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>3.121400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>3.144700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▆▆▅▅▄▅▅▄▄▄▃▄▄▃▃▃▂▂▃▂▂▁▂▂▂▂▂▂▂▂▁▁▂▁▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>432</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>3.1447</td></tr><tr><td>train/total_flos</td><td>225756315648000.0</td></tr><tr><td>train/train_loss</td><td>3.4303</td></tr><tr><td>train/train_runtime</td><td>181.0703</td></tr><tr><td>train/train_samples_per_second</td><td>2.386</td></tr><tr><td>train/train_steps_per_second</td><td>2.386</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">rich-water-21</strong>: <a href=\"https://wandb.ai/qulxis/my-awesome-project/runs/2q8nhdyb\" target=\"_blank\">https://wandb.ai/qulxis/my-awesome-project/runs/2q8nhdyb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221212_011403-2q8nhdyb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "id": "SFACIGBSV-EF"
   },
   "outputs": [],
   "source": [
    "trainer.model.config.task_specific_params['text-generation'] = {\n",
    "    'do_sample': True,\n",
    "    'min_length': 15,\n",
    "    'max_length': 100,\n",
    "    'temperature': 100,\n",
    "    'top_p': 0.95,\n",
    "    'prefix': '<|endoftext|>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzWHyukjUtBA",
    "outputId": "59381c3d-7678-499f-dfaf-a6a5a8ad61d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./model_files\n",
      "Configuration saved in ./model_files/config.json\n",
      "Model weights saved in ./model_files/pytorch_model.bin\n",
      "tokenizer config file saved in ./model_files/tokenizer_config.json\n",
      "Special tokens file saved in ./model_files/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#Save Model:\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whHzHwRfjVRS",
    "outputId": "80d0ae49-a4f3-4b9d-bf20-f42e0ee2d59f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.2406, 'learning_rate': 0.00013715465613551068, 'epoch': 0.01, 'step': 5}\n"
     ]
    }
   ],
   "source": [
    "a = trainer.state.log_history\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"If you, as a child, received a rabbit foot keychain, what year were you born? I'm trying to figure out when we stopped giving children random animal parts.\""
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Import opposite politeness set:\n",
    "#Tokenize:\n",
    "file_path = \"data/polite_hankgreen_50000_0.5_finalnum3541.txt\"\n",
    "#Polite:\n",
    "# polite_hankgreen_50000_0.5_finalnum3541.txt\n",
    "#Impolite:\n",
    "# impolite_SenSanders_23400_0.5_finalnum9578.txt\n",
    "# impolite_hankgreen_50000_0.25_finalnum7746.txt\n",
    "with open(file_path) as f:\n",
    "    lines = f.readlines()\n",
    "lines = lines[0].split(\"<|endoftext|>\")\n",
    "lines[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1HxL5c5U3Xz"
   },
   "source": [
    "# Predict:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQFrMXY1XvNM",
    "outputId": "ba4b7ffe-5e7b-4a87-eb7a-a94aded2d62f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/ecbm4040/envTF24/lib/python3.6/site-packages/transformers/generation_beam_search.py:197: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  \"Passing `max_length` to BeamSearchScorer is deprecated and has no effect. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear John, I just want you to know that I love you so much.\n",
      "Dear John, I just want you to know that I love you.\n",
      "Dear John, I just want you to know that I love you, John.\n",
      "Dear John, I just want you to know that I love you so much. I am so sorry.\n",
      "Dear John, I think it's time for you to get your ass out of the house.\n",
      "Dear John, I think it's time for you to get your ass out of here.\n",
      "Dear John, I just want you to know that I love you so much. I am so sorry about that.\n",
      "Dear John, I think it's time for you to get out of the house.\n",
      "Dear John, I think it's time for you to get your ass out there and do something good.\n",
      "Dear John, I just want you to know that I love you so much. I am so sorry about this.\n"
     ]
    }
   ],
   "source": [
    "# prepare input\n",
    "predictions = []\n",
    "start = \"Dear John,\"\n",
    "\n",
    "\"\"\"\n",
    "Notes:\n",
    "Temperature: trade-off between variety and politeness clarity\n",
    "Beam Search vs Top-k/Top-p. Beam search is a lot more coherent with a trade off for variety.\n",
    "\n",
    "Options:\n",
    "\n",
    "Naive Beam Search: Num_beams = 10, all else off\n",
    "Top K with Nucleus Sampling: top_p = 0.95, top_k = 10-20, do_sample=True\n",
    "Beam-search multinomial sampling : Num_beams = 10 + do_sample = True\n",
    "Diverse beam-search decoding: Num_beams = 10 + num_beam_groups = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "start_with_bos = '<|endoftext|>' + start\n",
    "encoded_prompt = trainer.tokenizer(start_with_bos, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "encoded_prompt = encoded_prompt.to(trainer.model.device)\n",
    "\n",
    "# prediction\n",
    "output_sequences = trainer.model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    max_length=160, #originally 160\n",
    "    min_length=10, #originally 10\n",
    "    temperature= 0.75, #originally 1\n",
    "    no_repeat_ngram_size=2,    \n",
    "    \n",
    "    \n",
    "    #Edit stuff here down to change decoding \n",
    "# BEAM\n",
    "#     num_beams=10, #on or off\n",
    "\n",
    "# TOP    \n",
    "#     do_sample=True, # for multinomial beam search and top sampling\n",
    "#     top_p = 0.95, #0.95\n",
    "#     top_k = 10, #10-20\n",
    "\n",
    "# MULTINOMIAL\n",
    "#     num_beams=10, #on or off\n",
    "#     do_sample=True, # for multinomial beam search and top sampling\n",
    "\n",
    "# DIVERSE \n",
    "#     num_beams=10, #on or off  \n",
    "#     num_beam_groups = 2, # on or off, must be a multiple of num_beams\n",
    "\n",
    "    \n",
    "    \n",
    "    num_return_sequences= 10, #must = num_beam for diverse beam-search\n",
    "    \n",
    "    )\n",
    "generated_sequences = []\n",
    "\n",
    "# decode prediction\n",
    "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    text = trainer.tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
    "    if not ALLOW_NEW_LINES:\n",
    "        limit = text.find('\\n')\n",
    "        text = text[: limit if limit != -1 else None]\n",
    "    generated_sequences.append(text.strip())\n",
    "\n",
    "for i, g in enumerate(generated_sequences):\n",
    "    predictions.append([start, g])\n",
    "\n",
    "for pair in predictions:\n",
    "  print(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning\n"
     ]
    }
   ],
   "source": [
    "print(\"warning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Target      Prompt                                             Tweets  \\\n",
      "0  hankgreen  Dear John,  Dear John, I just want you to know that I love...   \n",
      "1  hankgreen  Dear John,  Dear John, I just want you to know that I love...   \n",
      "2  hankgreen  Dear John,  Dear John, I just want you to know that I love...   \n",
      "3  hankgreen  Dear John,  Dear John, I just want you to know that I love...   \n",
      "4  hankgreen  Dear John,  Dear John, I think it's time for you to get yo...   \n",
      "5  hankgreen  Dear John,  Dear John, I think it's time for you to get yo...   \n",
      "6  hankgreen  Dear John,  Dear John, I just want you to know that I love...   \n",
      "7  hankgreen  Dear John,  Dear John, I think it's time for you to get ou...   \n",
      "8  hankgreen  Dear John,  Dear John, I think it's time for you to get yo...   \n",
      "9  hankgreen  Dear John,  Dear John, I just want you to know that I love...   \n",
      "\n",
      "     Type  \n",
      "0  polite  \n",
      "1  polite  \n",
      "2  polite  \n",
      "3  polite  \n",
      "4  polite  \n",
      "5  polite  \n",
      "6  polite  \n",
      "7  polite  \n",
      "8  polite  \n",
      "9  polite  \n"
     ]
    }
   ],
   "source": [
    "username = \"hankgreen\" #twitter user\n",
    "type_tweet = \"polite\" #polite or impolite\n",
    "decoder = \"diverse\" #beam, top (top_k + top_p), multinomial (Num_beams + do_sample), diverse (Num_beams + num_beam_groups)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns = [\"Target\",\"Prompt\",\"Tweets\",\"Type\"])\n",
    "target_col = [username]*len(predictions) # target col\n",
    "type_col = [type_tweet]*len(predictions) # type col\n",
    "prompt_col = []\n",
    "tweets_col = []\n",
    "\n",
    "for pair in predictions:\n",
    "    prompt_col.append(pair[0])\n",
    "    tweets_col.append(pair[1])\n",
    "\n",
    "df[\"Target\"] = target_col\n",
    "df[\"Prompt\"] = prompt_col\n",
    "df[\"Tweets\"] = tweets_col\n",
    "df[\"Type\"] = type_col\n",
    "df.reset_index()\n",
    "# print(df)\n",
    "\n",
    "df.to_csv('responses/{}_{}_{}.csv'.format(username,type_tweet,decoder), index=False)\n",
    "#check writing:\n",
    "\n",
    "df_test = pd.read_csv('responses/{}_{}_{}.csv'.format(username,type_tweet,decoder))\n",
    "print(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Target, Tweets, Type]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBQuSNhMTpAT"
   },
   "source": [
    "# New Section\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02e007fb503c4abea13fcdab99ba672c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfb9c954923747d586a85fc1cf1442dc",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_98e279404402473783c5dafc59e78b58",
      "value": 0.002181832796966104
     }
    },
    "04f58430b59b4df8b5352bda202b47f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "753de470647f4d58ab2454117f2610b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f2e1d44a4bd491f91db7a44a5fa0938",
       "IPY_MODEL_02e007fb503c4abea13fcdab99ba672c"
      ],
      "layout": "IPY_MODEL_9c1cdb3f8f504888ae1947271af1061f"
     }
    },
    "7fe9e859d70e4399bf8e61f1382ccdd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f2e1d44a4bd491f91db7a44a5fa0938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04f58430b59b4df8b5352bda202b47f9",
      "placeholder": "​",
      "style": "IPY_MODEL_7fe9e859d70e4399bf8e61f1382ccdd3",
      "value": "0.001 MB of 0.332 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "98e279404402473783c5dafc59e78b58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9c1cdb3f8f504888ae1947271af1061f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfb9c954923747d586a85fc1cf1442dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
