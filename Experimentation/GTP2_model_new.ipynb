{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-2 FINE TUNING AND GENERATION"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets install the required packages. If runing on colab, include the following pip commands after connecting to runtime and connecting your google drive. Use the torch install option (current commented out) if these depreciate. To use TPUs, uncomment the last line as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTsPagZoU-bL",
    "outputId": "ec7d7bd6-6d34-4873-ad48-bd04766a345b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers 'C:\\Users\\andre\\AppData\\Local\\Temp\\pip-req-build-dcj1apwe'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Cloning https://github.com/huggingface/transformers to c:\\users\\andre\\appdata\\local\\temp\\pip-req-build-dcj1apwe\n",
      "  Resolved https://github.com/huggingface/transformers to commit d994473b05a83ea398d9f10ca458855df095e22d\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.26.0.dev0) (0.12.1)\n",
      "Requirement already satisfied: requests in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.26.0.dev0) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.26.0.dev0) (2022.10.31)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.26.0.dev0) (1.23.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.26.0.dev0) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.26.0.dev0) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.26.0.dev0) (0.11.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.26.0.dev0) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from transformers==4.26.0.dev0) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.26.0.dev0) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging>=20.0->transformers==4.26.0.dev0) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.27->transformers==4.26.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==4.26.0.dev0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==4.26.0.dev0) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==4.26.0.dev0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests->transformers==4.26.0.dev0) (1.26.12)\n",
      "Requirement already satisfied: wandb in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.13.6)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\andre\\appdata\\roaming\\python\\python39\\site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: promise<3,>=2.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: pathtools in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (1.11.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (58.1.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
      "     ---------------------------------------- 81.4/81.4 kB 4.4 MB/s eta 0:00:00\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "     ---------------------------------------- 132.9/132.9 kB ? eta 0:00:00\n",
      "Requirement already satisfied: dill in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (0.3.1.1)\n",
      "Collecting datasets>=2.0.0\n",
      "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
      "     -------------------------------------- 451.7/451.7 kB 7.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (0.11.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (4.64.1)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.11.0-py3-none-any.whl (139 kB)\n",
      "     -------------------------------------- 139.5/139.5 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (1.23.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (1.5.2)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.1.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from evaluate) (2.28.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.3-cp39-cp39-win_amd64.whl (323 kB)\n",
      "     -------------------------------------- 323.5/323.5 kB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-10.0.1-cp39-cp39-win_amd64.whl (20.3 MB)\n",
      "     --------------------------------------- 20.3/20.3 MB 11.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from packaging->evaluate) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     -------------------------------------- 110.5/110.5 kB 6.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas->evaluate) (2022.6)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.3-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\andre\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, fsspec, frozenlist, dill, yarl, responses, multiprocess, aiosignal, aiohttp, datasets, evaluate\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.1.1\n",
      "    Uninstalling dill-0.3.1.1:\n",
      "      Successfully uninstalled dill-0.3.1.1\n",
      "Successfully installed aiohttp-3.8.3 aiosignal-1.3.1 datasets-2.7.1 dill-0.3.6 evaluate-0.4.0 frozenlist-1.3.3 fsspec-2022.11.0 multidict-6.0.3 multiprocess-0.70.14 pyarrow-10.0.1 responses-0.18.0 xxhash-3.1.0 yarl-1.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install wandb\n",
    "!pip install evaluate\n",
    "\n",
    "# For special cases and TPU use run:\n",
    "# !pip install torch transformers wandb -qqq \n",
    "# !pip install cloud-tpu-client==0.10 torch==1.13.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.13-cp38-cp38-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "PmcAtJ61ToFn",
    "outputId": "a98a7842-d346-418e-8fe6-7b2edf3adb75"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import (\n",
    "            AutoTokenizer, AutoModelForCausalLM,\n",
    "            TextDataset, DataCollatorForLanguageModeling,\n",
    "            Trainer, TrainingArguments,\n",
    "            get_cosine_schedule_with_warmup,\n",
    "            EarlyStoppingCallback,  IntervalStrategy)\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import pathlib\n",
    "import random\n",
    "import evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING STEPS\n",
    "The fololow requires .txt files in the appropriate formate generated by either \"Scrapping and Cleaning.ipynb\" or/and \"full_model_processing.ipynb\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have your wandb account setup to access the training evaluations. \n",
    "# You will need to have your token ready. If no popup appears (ie running this in VSCode) run this first in terminal\n",
    "import wandb\n",
    "wandb.init(project=\"my-awesome-project\")\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load in our data and update the tokenizer. We can include new special tokens to the library if required by setting \"use_special_tokens\" to True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlnX7c1OUl0v",
    "outputId": "0227d49b-3cb0-403c-f21c-79c0e219677d"
   },
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# SETUP:\n",
    "# Select the text file to use for training and evaluation set for monitoring our progress\n",
    "file_path =  \"data/special_token_versions/textfiles/train_data.txt\"  #Training Set. Use train_data.txt for full scale\n",
    "file_path_val = \"data/special_token_versions/textfiles/test_data.txt\" # Test Set. Use test_data.txt for full scale\n",
    "use_special_tokens = True # use custom tokens or no\n",
    "\n",
    "#Add custom tokens explicitly:\n",
    "special_tokens_manual= ['<|sensander|>', '<|paddingtonbear|>', '<|hankgreen|>', '<|joerogan|>', '<|elonmusk|>', '<|polite|>', '<|impolite|>','<|neutral|>']\n",
    "#Add tokens from csv file:\n",
    "token_file_path = \"data/special_token_versions/keys.csv\"\n",
    "#####################################################################################\n",
    "\n",
    "\n",
    "new_special_tokens = pd.read_csv(token_file_path)[\"Keys\"].values.tolist() + special_tokens_manual #add these to special tokens\n",
    "special_tokens_dict = {'additional_special_tokens': new_special_tokens} # use for direct add\n",
    "\"\"\"\n",
    "Here, we specify what we train we use to train our data. In our case, we\n",
    "have several special tokens (ones that shouldn't be split). \n",
    "\n",
    "We include subject tokens, user tokens, and politeness tokens manually in special_tokens_manual\n",
    "\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate Model and Tokenizer. Updates the tokenizer and model to use special tags if selected for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in base GPT-2 model and corresponding tokenizer:\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "#Add special tokens and update model if required:\n",
    "if use_special_tokens:\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict) #adds special tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "block_size = tokenizer.model_max_length\n",
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=file_path, block_size=block_size, overwrite_cache=True)\n",
    "evaluation_dataset = TextDataset(tokenizer=tokenizer, file_path=file_path_val, block_size=block_size, overwrite_cache=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "#setup wanbi:\n",
    "%env WANDB_PROJECT=tweet_analysis\n",
    "\n",
    "wandb.run.name = file_path\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N75aQ7ucUoDD",
    "outputId": "eebc6dbf-8d38-425c-fdf2-3125b7aba852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 13 23:29:59 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 526.98       Driver Version: 526.98       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   57C    P8    N/A /  N/A |    184MiB /  2048MiB |     21%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     43840    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Check that you have a GPU connected\n",
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next define our parameters. We use an early stopping callback metric that uses a compute_metrics function which can be changed to use different metrics (ie accuracy, precision, f1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6tkauYiUpct",
    "outputId": "b13ea060-cf9f-49b3-d990-b1228971f57a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# START: COPIED FROM <https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb#scrollTo=ZSCf6QyF8AG- >\n",
    "ALLOW_NEW_LINES = False     # seems to work better <--- from source\n",
    "LEARNING_RATE = 1.372e-4\n",
    "EPOCHS = 4\n",
    "seed = random.randint(0,2**32-1)\n",
    "# END: COPIED FROM <https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb#scrollTo=ZSCf6QyF8AG- >\n",
    "\n",
    "import evaluate\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"wandb\", #Remove if not using wandb \n",
    "    output_dir=\"./model_files\", #change this to new location if exisitng /model_files folder exists. Will overwrite otherwise\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    evaluation_strategy = 'steps',# num_train_epochs=1, #new\n",
    "    eval_steps = 5000, #\n",
    "    per_device_train_batch_size=1,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=5,\n",
    "    save_steps=0,\n",
    "    seed=seed,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    metric_for_best_model = 'f1',#new\n",
    "    load_best_model_at_end = True, #new\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset = evaluation_dataset,\n",
    "    compute_metrics=compute_metrics, #new\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]) #new\n",
    "\n",
    "# START: COPIED FROM <https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb#scrollTo=ZSCf6QyF8AG- >\n",
    "\n",
    "#LR schedule stuff?\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "num_train_steps = len(train_dataloader)\n",
    "trainer.create_optimizer_and_scheduler(num_train_steps)\n",
    "trainer.lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    trainer.optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_train_steps)\n",
    "# END: COPIED FROM <https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb#scrollTo=ZSCf6QyF8AG- >\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "753de470647f4d58ab2454117f2610b3",
      "8f2e1d44a4bd491f91db7a44a5fa0938",
      "02e007fb503c4abea13fcdab99ba672c",
      "9c1cdb3f8f504888ae1947271af1061f",
      "04f58430b59b4df8b5352bda202b47f9",
      "7fe9e859d70e4399bf8e61f1382ccdd3",
      "cfb9c954923747d586a85fc1cf1442dc",
      "98e279404402473783c5dafc59e78b58"
     ]
    },
    "id": "Y2RvzAyfUrNn",
    "outputId": "08be6368-6aa1-4565-d940-51a7f0fdae70"
   },
   "outputs": [],
   "source": [
    "#Train new model\n",
    "trainer.train()\n",
    "wandb.finish() #Exit wandb recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SFACIGBSV-EF"
   },
   "outputs": [],
   "source": [
    "#configure model task\n",
    "trainer.model.config.task_specific_params['text-generation'] = {\n",
    "    'do_sample': True,\n",
    "    'min_length': 15,\n",
    "    'max_length': 100,\n",
    "    'temperature': 100,\n",
    "    'top_p': 0.95,\n",
    "    'prefix': '<|endoftext|>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzWHyukjUtBA",
    "outputId": "59381c3d-7678-499f-dfaf-a6a5a8ad61d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./model_files\n",
      "Configuration saved in ./model_files/config.json\n",
      "Model weights saved in ./model_files/pytorch_model.bin\n",
      "tokenizer config file saved in ./model_files/tokenizer_config.json\n",
      "Special tokens file saved in ./model_files/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#Save Model:\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whHzHwRfjVRS",
    "outputId": "80d0ae49-a4f3-4b9d-bf20-f42e0ee2d59f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9403, 'learning_rate': 0.0001371792092297936, 'epoch': 0.01, 'step': 5}\n"
     ]
    }
   ],
   "source": [
    "#Example to view training history if wandb not used\n",
    "a = trainer.state.log_history\n",
    "print(a[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE EXISTING MODEL\n",
    "Load our trained models and tokenizers. Set tokenizer and model to folder the fine-tuned model is in in the first two lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################\n",
    "#To load models\n",
    "tokenizer = AutoTokenizer.from_pretrained('./model_files_no_special_tokens') #change to match as needed\n",
    "model = AutoModelForCausalLM.from_pretrained('./model_files_no_special_tokens') #chnage to mtach as needed\n",
    "###########################################################################################################\n",
    "\n",
    "\n",
    "ALLOW_NEW_LINES = False     \n",
    "LEARNING_RATE = 1.372e-4\n",
    "seed = random.randint(0,2**32-1)\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./model_files2\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    evaluation_strategy = IntervalStrategy.STEPS, # num_train_epochs=1, #new\n",
    "    eval_steps = 1000, #new\n",
    "    num_train_epochs=1,\n",
    "    save_total_limit = 10, \n",
    "    per_device_train_batch_size=1,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=5,\n",
    "    save_steps=0,\n",
    "    seed=seed,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    metric_for_best_model = None,#new\n",
    "    load_best_model_at_end = True)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=None) #new\n",
    "  \n",
    "trainer.model.config.task_specific_params['text-generation'] = {\n",
    "    'do_sample': True,\n",
    "    'min_length': 10,\n",
    "    'max_length': 160,\n",
    "    'temperature': 1.,\n",
    "    'top_p': 0.95,\n",
    "    'prefix': '<|endoftext|>'}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "H1HxL5c5U3Xz"
   },
   "source": [
    "# Predict:\n",
    "Generate predictions. We can change our control parameters as well as the decoding methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQFrMXY1XvNM",
    "outputId": "ba4b7ffe-5e7b-4a87-eb7a-a94aded2d62f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/ecbm4040/envTF24/lib/python3.6/site-packages/transformers/generation_beam_search.py:197: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  \"Passing `max_length` to BeamSearchScorer is deprecated and has no effect. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I think I'll get a megaphone like Mr Curry's to remind everyone.\n",
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I suggest preparing some marmalade sandwiches for an extra special elevenses!\n",
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I suggest preparing some marmalade sandwiches for an extra special elevenses!\n",
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I think I'll get a megaphone like Mr Curry's to remind everyone.\n",
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I think I'll get a meg megaphone like Mr Curry's to remind everyone.\n",
      "Mr. Brown, I’m very pleased that you would like me to send you a reminder about my new film #Paddington2. I think I'll get a megaphone like Mr Curry's to remind everyone.\n",
      "Mr. Brown, I’m very pleased that you would like me to send you a reminder about my new film #Paddington2. I suggest preparing some marmalade sandwiches for an extra special elevenses!\n",
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I think I'll get a megaphone like Mr Curry's to remind all of us.\n",
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I think I'll get a meg like Mr Curry's to share it with you all.\n",
      "Mr. Brown, I'm very pleased that you would like me to send a reminder about my new film #Paddington2. I suggest preparing some marmalade sandwiches for an extra special elevenses!\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# Control parameters. \n",
    "tag = \"<|elonmusk|>\" #options are <|sensander|>, <|elonmusk|>, <|hankgreen|>, <|elonmusk|>, <|joerogan|>,\n",
    "polite_tag = \"<|neutral|>\" #options are <|polite|>, <|neutral|>, <|impolite|>,\n",
    "topic1 = '<|spacex|>' # topic choice 1\n",
    "topic2 = '<|failure|>' # topic choice 2. To use one topic, set to <|undefined|>\n",
    "#########################################################################\n",
    "\n",
    "\"\"\"\n",
    "Notes:\n",
    "Temperature: trade-off between variety and politeness clarity\n",
    "Beam Search vs Top-k/Top-p. Beam search is a lot more coherent with a trade off for variety.\n",
    "\n",
    "Options:\n",
    "\n",
    "Naive Beam Search: Num_beams = 10, all else off\n",
    "Top K with Nucleus Sampling: top_p = 0.95, top_k = 10-20, do_sample=True\n",
    "Beam-search multinomial sampling : Num_beams = 10 + do_sample = True\n",
    "Diverse beam-search decoding: Num_beams = 10 + num_beam_groups = 2\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "start = \"\"\n",
    "predictions = []\n",
    "start_with_bos = '<|endoftext|>'+tag+polite_tag+topic1+topic2 + start\n",
    "encoded_prompt = trainer.tokenizer(start_with_bos, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "encoded_prompt = encoded_prompt.to(trainer.model.device)\n",
    "\n",
    "\n",
    "output_sequences = trainer.model.generate(\n",
    "###################################################################################  \n",
    "# We can alter our how our model's decoding strategies here: \n",
    "    #Edit stuff here down to change decoding \n",
    "# BEAM Naive (uncomment below to use)\n",
    "    # num_beams=10, #on or off\n",
    "\n",
    "# TOP-K + Nucleus (uncomment below to use)\n",
    "    do_sample=True, # for multinomial beam search and top sampling\n",
    "    top_p = 0.95, #0.95\n",
    "    top_k = 50, #10-20\n",
    "\n",
    "# MULTINOMIAL BEAM SEARCH (uncomment below to use)\n",
    "    # num_beams=10, #on or off\n",
    "    # do_sample=True, # for multinomial beam search and top sampling\n",
    "\n",
    "# DIVERSE BEAM SEARCH (uncomment below to use)\n",
    "    # num_beams=10, #on or off  \n",
    "    # num_beam_groups = 2, # on or off, must be a multiple of num_beams\n",
    "#####################################################################################\n",
    "    \n",
    "    \n",
    "    num_return_sequences= 10, #must = num_beam for diverse beam-search\n",
    "    input_ids=encoded_prompt,\n",
    "    max_length=160, #originally 160\n",
    "    min_length=10, #originally 10\n",
    "    temperature = 1, #originally 1\n",
    "    no_repeat_ngram_size=2,   \n",
    "    \n",
    "    )\n",
    "# START: COPIED FROM <https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb#scrollTo=ZSCf6QyF8AG- >\n",
    "\n",
    "generated_sequences = []\n",
    "\n",
    "# decode prediction\n",
    "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    text = trainer.tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
    "    if not ALLOW_NEW_LINES:\n",
    "        limit = text.find('\\n')\n",
    "        text = text[: limit if limit != -1 else None]\n",
    "    generated_sequences.append(text.strip())\n",
    "\n",
    "for i, g in enumerate(generated_sequences):\n",
    "    predictions.append([start, g])\n",
    "# END: COPIED FROM <https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb#scrollTo=ZSCf6QyF8AG- >\n",
    "\n",
    "\n",
    "for pair in predictions:\n",
    "  print(pair[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE AS CSV\n",
    "If we like our set of generation, we can save it to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note which decoding algorthim you used below before running\n",
    "decoder = \"diverse\" #beam, top (top_k + top_p), multinomial (Num_beams + do_sample), diverse (Num_beams + num_beam_groups)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "username = tag #twitter user\n",
    "type_tweet = polite_tag #polite, impolite, neutral\n",
    "topics = topic1+topic2\n",
    "\n",
    "\n",
    "df = pd.DataFrame(columns = [\"Target\",\"Prompt\",\"Tweets\",\"Type\"])\n",
    "target_col = [username]*len(predictions) # target col\n",
    "type_col = [type_tweet]*len(predictions) # type col\n",
    "prompt_col = []\n",
    "tweets_col = []\n",
    "\n",
    "for pair in predictions:\n",
    "    prompt_col.append(pair[0])\n",
    "    tweets_col.append(pair[1])\n",
    "\n",
    "df[\"Target\"] = target_col\n",
    "df[\"Prompt\"] = prompt_col\n",
    "df[\"Tweets\"] = tweets_col\n",
    "df[\"Type\"] = type_col\n",
    "df.reset_index()\n",
    "# print(df)\n",
    "\n",
    "df.to_csv('responses/{}_{}_{}.csv'.format(username,type_tweet,decoder), index=False)\n",
    "#check writing:\n",
    "\n",
    "df_test = pd.read_csv('responses/{}_{}_{}.csv'.format(username,type_tweet,decoder))\n",
    "# print(df_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "08e595c52ca3b9470036b1110e67b559e55f367cabc363f2e28d35631ed95060"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02e007fb503c4abea13fcdab99ba672c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfb9c954923747d586a85fc1cf1442dc",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_98e279404402473783c5dafc59e78b58",
      "value": 0.002181832796966104
     }
    },
    "04f58430b59b4df8b5352bda202b47f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "753de470647f4d58ab2454117f2610b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f2e1d44a4bd491f91db7a44a5fa0938",
       "IPY_MODEL_02e007fb503c4abea13fcdab99ba672c"
      ],
      "layout": "IPY_MODEL_9c1cdb3f8f504888ae1947271af1061f"
     }
    },
    "7fe9e859d70e4399bf8e61f1382ccdd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f2e1d44a4bd491f91db7a44a5fa0938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04f58430b59b4df8b5352bda202b47f9",
      "placeholder": "​",
      "style": "IPY_MODEL_7fe9e859d70e4399bf8e61f1382ccdd3",
      "value": "0.001 MB of 0.332 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "98e279404402473783c5dafc59e78b58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9c1cdb3f8f504888ae1947271af1061f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfb9c954923747d586a85fc1cf1442dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
