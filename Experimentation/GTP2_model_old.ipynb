{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTsPagZoU-bL",
    "outputId": "ec7d7bd6-6d34-4873-ad48-bd04766a345b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-zu2v4qyr\n",
      "  Running command git clone -q https://github.com/huggingface/transformers /tmp/pip-req-build-zu2v4qyr\n",
      "  Resolved https://github.com/huggingface/transformers to commit 799cea64ac1029d66e9e58f18bc6f47892270723\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement huggingface-hub<1.0,>=0.10.0 (from transformers) (from versions: 0.0.1, 0.0.2, 0.0.3rc1, 0.0.3rc2, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.9, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.0.14, 0.0.15, 0.0.16, 0.0.17, 0.0.18, 0.0.19, 0.1.0, 0.1.1, 0.1.2, 0.2.0, 0.2.1, 0.4.0)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for huggingface-hub<1.0,>=0.10.0\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ecbm4040/envTF24/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wandb in /home/ecbm4040/envTF24/lib/python3.6/site-packages (0.13.6)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (3.1.18)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (3.17.3)\n",
      "Requirement already satisfied: PyYAML in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (1.11.1)\n",
      "Requirement already satisfied: pathtools in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (58.0.2)\n",
      "Requirement already satisfied: setproctitle in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: dataclasses in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (0.8)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: importlib-metadata in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.8.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ecbm4040/envTF24/lib/python3.6/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.5.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ecbm4040/envTF24/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EHvMfwd3iz3i"
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "PmcAtJ61ToFn",
    "outputId": "a98a7842-d346-418e-8fe6-7b2edf3adb75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mqulxis\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ecbm4040/StyleTransfer_SpeechGen/Experimentation/wandb/run-20221212_070236-22prx3db</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/qulxis/my-awesome-project/runs/22prx3db\" target=\"_blank\">silver-dawn-35</a></strong> to <a href=\"https://wandb.ai/qulxis/my-awesome-project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import (\n",
    "            AutoTokenizer, AutoModelForCausalLM,\n",
    "            TextDataset, DataCollatorForLanguageModeling,\n",
    "            Trainer, TrainingArguments,\n",
    "            get_cosine_schedule_with_warmup)\n",
    "from transformers import EarlyStoppingCallback\n",
    "# from huggingface_hub.hf_api import HfAp\n",
    "# import api.filter\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "import wandb\n",
    "wandb.init(project=\"my-awesome-project\")\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlnX7c1OUl0v",
    "outputId": "0227d49b-3cb0-403c-f21c-79c0e219677d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecbm4040/envTF24/lib/python3.6/site-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (653573 > 1024). Running this sequence through the model will result in indexing errors\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=tweet_analysis\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize:\n",
    "file_path =  \"data/special_token_versions/textfiles/all_data.txt\"  \n",
    "use_special_tokens = True # use custom tokens\n",
    "special_tokens_manual= ['<|sensander|>', '<|paddingtonbear|>', '<|hankgreen|>', '<|joerogan|>', '<|elonmusk|>', '<|polite|>', '<|impolite|>','<|neutral|>']\n",
    "#read new tokens:\n",
    "token_file_path = \"data/special_token_versions/keys.csv\"\n",
    "new_special_tokens = pd.read_csv(token_file_path)[\"Keys\"].values.tolist() + special_tokens_manual #add these to special tokens\n",
    "special_tokens_dict = {'additional_special_tokens': new_special_tokens} # use for direct add\n",
    "wandb.run.name = file_path\n",
    "\"\"\"\n",
    "Here, we specify what we train we use to train our data. In our case, we\n",
    "have several special tokens (ones that shouldn't be split). \n",
    "\n",
    "We include subject tokens, user tokens, and politeness tokens manually in special_tokens_manual\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "#add special tokens:\n",
    "if use_special_tokens:\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict) #adds special tokens\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "block_size = tokenizer.model_max_length\n",
    "train_dataset = TextDataset(tokenizer=tokenizer, file_path=file_path, block_size=block_size, overwrite_cache=True)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "#setup wanbi:\n",
    "%env WANDB_PROJECT=tweet_analysis\n",
    "\n",
    "wandb.run.name = file_path\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N75aQ7ucUoDD",
    "outputId": "eebc6dbf-8d38-425c-fdf2-3125b7aba852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Dec 12 07:02:48 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   77C    P0    70W /  70W |    207MiB / 15109MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1211      C   python2.9                         101MiB |\n",
      "|    0   N/A  N/A      1405      C   ./nanom                           103MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6tkauYiUpct",
    "outputId": "b13ea060-cf9f-49b3-d990-b1228971f57a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecbm4040/envTF24/lib/python3.6/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "#Train:\n",
    "#Largely taken from: https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb#scrollTo=ZSCf6QyF8AG-\n",
    "\n",
    "ALLOW_NEW_LINES = False     # seems to work better <--- from source\n",
    "LEARNING_RATE = 1.372e-4\n",
    "EPOCHS = 4\n",
    "seed = random.randint(0,2**32-1)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"./model_files\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    evaluation_strategy = 'steps',# num_train_epochs=1, #new\n",
    "    eval_steps = 50, #new\n",
    "    per_device_train_batch_size=1,\n",
    "    prediction_loss_only=True,\n",
    "    logging_steps=5,\n",
    "    save_steps=0,\n",
    "    seed=seed,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    metric_for_best_model = 'f1',#new\n",
    "    load_best_model_at_end = True, #new\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    compute_metrics=compute_metrics, #new\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]) #new\n",
    "#LR schedule stuff?\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "num_train_steps = len(train_dataloader)\n",
    "trainer.create_optimizer_and_scheduler(num_train_steps)\n",
    "trainer.lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    trainer.optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_train_steps)\n",
    "\n",
    "# p_start, p_end = 0.4, 0.9\n",
    "# def progressify(f):\n",
    "#     \"Control progress bar when calling f\"\n",
    "#     def inner(*args, **kwargs):\n",
    "#         if trainer.state.epoch is not None:\n",
    "#             # we only have one epoch, EPOCHS is built into dataset\n",
    "#             progress.value = p_start + trainer.state.epoch * (p_end - p_start)\n",
    "#         return f(*args, **kwargs)\n",
    "#     return inner\n",
    "\n",
    "# trainer.training_step = progressify(trainer.training_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "753de470647f4d58ab2454117f2610b3",
      "8f2e1d44a4bd491f91db7a44a5fa0938",
      "02e007fb503c4abea13fcdab99ba672c",
      "9c1cdb3f8f504888ae1947271af1061f",
      "04f58430b59b4df8b5352bda202b47f9",
      "7fe9e859d70e4399bf8e61f1382ccdd3",
      "cfb9c954923747d586a85fc1cf1442dc",
      "98e279404402473783c5dafc59e78b58"
     ]
    },
    "id": "Y2RvzAyfUrNn",
    "outputId": "08be6368-6aa1-4565-d940-51a7f0fdae70"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 638\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 638\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='638' max='638' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [638/638 08:57, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.940300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.112700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.770300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.959900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.804100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.960600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.737900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>1.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.535900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.835400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1.896200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.635400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>1.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.750500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>1.744400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.737600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>1.639600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.665600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.760400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.664300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>1.446800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>1.529100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.719800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>1.619900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>1.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.727200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.477900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.622200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>1.359800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>1.624400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.433100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>1.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.583700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>1.540400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.621300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.562700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>1.509200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>1.506800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.507600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.371300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>1.466100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.568200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.514000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>1.444700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.419300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>1.430400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>1.367900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.387400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>1.397900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.328000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.353700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.454000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>1.378200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.505300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>1.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.423200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>1.341500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>1.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>1.421200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.432200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>1.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.403900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>1.413500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>1.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.361900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.406300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.166500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>1.336500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.300600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>1.387700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>1.246600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.306500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>1.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>1.308500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>1.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>1.340900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.263200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>1.335400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.351300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>1.381100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.342500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>1.260900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>1.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.205700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>1.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>1.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>1.219900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>1.471900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>1.295600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.269700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>1.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.274600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>1.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>1.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>1.238700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.299500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>1.254700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>███████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▃▄▃▃▃▃▂▃▃▂▂▂▂▂▂▂▁▂▁▁▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>638</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.2547</td></tr><tr><td>train/total_flos</td><td>333408632832000.0</td></tr><tr><td>train/train_loss</td><td>1.48939</td></tr><tr><td>train/train_runtime</td><td>538.3123</td></tr><tr><td>train/train_samples_per_second</td><td>1.185</td></tr><tr><td>train/train_steps_per_second</td><td>1.185</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">silver-dawn-35</strong>: <a href=\"https://wandb.ai/qulxis/my-awesome-project/runs/22prx3db\" target=\"_blank\">https://wandb.ai/qulxis/my-awesome-project/runs/22prx3db</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221212_070236-22prx3db/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SFACIGBSV-EF"
   },
   "outputs": [],
   "source": [
    "trainer.model.config.task_specific_params['text-generation'] = {\n",
    "    'do_sample': True,\n",
    "    'min_length': 15,\n",
    "    'max_length': 100,\n",
    "    'temperature': 100,\n",
    "    'top_p': 0.95,\n",
    "    'prefix': '<|endoftext|>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzWHyukjUtBA",
    "outputId": "59381c3d-7678-499f-dfaf-a6a5a8ad61d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./model_files\n",
      "Configuration saved in ./model_files/config.json\n",
      "Model weights saved in ./model_files/pytorch_model.bin\n",
      "tokenizer config file saved in ./model_files/tokenizer_config.json\n",
      "Special tokens file saved in ./model_files/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#Save Model:\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "whHzHwRfjVRS",
    "outputId": "80d0ae49-a4f3-4b9d-bf20-f42e0ee2d59f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9403, 'learning_rate': 0.0001371792092297936, 'epoch': 0.01, 'step': 5}\n"
     ]
    }
   ],
   "source": [
    "a = trainer.state.log_history\n",
    "print(a[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1HxL5c5U3Xz"
   },
   "source": [
    "# Predict:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQFrMXY1XvNM",
    "outputId": "ba4b7ffe-5e7b-4a87-eb7a-a94aded2d62f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/ecbm4040/envTF24/lib/python3.6/site-packages/transformers/generation_beam_search.py:197: UserWarning: Passing `max_length` to BeamSearchScorer is deprecated and has no effect. `max_length` should be passed directly to `beam_search(...)`, `beam_sample(...)`, or `group_beam_search(...)`.\n",
      "  \"Passing `max_length` to BeamSearchScorer is deprecated and has no effect. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I think I'll get a megaphone like Mr Curry's to remind everyone.\n",
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I suggest preparing some marmalade sandwiches for an extra special elevenses!\n",
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I suggest preparing some marmalade sandwiches for an extra special elevenses!\n",
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I think I'll get a megaphone like Mr Curry's to remind everyone.\n",
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I think I'll get a meg megaphone like Mr Curry's to remind everyone.\n",
      "Mr. Brown, I’m very pleased that you would like me to send you a reminder about my new film #Paddington2. I think I'll get a megaphone like Mr Curry's to remind everyone.\n",
      "Mr. Brown, I’m very pleased that you would like me to send you a reminder about my new film #Paddington2. I suggest preparing some marmalade sandwiches for an extra special elevenses!\n",
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I think I'll get a megaphone like Mr Curry's to remind all of us.\n",
      "Mr. Brown, I'm very pleased that you would like me to send you a reminder about my new film #Paddington2. I think I'll get a meg like Mr Curry's to share it with you all.\n",
      "Mr. Brown, I'm very pleased that you would like me to send a reminder about my new film #Paddington2. I suggest preparing some marmalade sandwiches for an extra special elevenses!\n"
     ]
    }
   ],
   "source": [
    "# prepare input\n",
    "predictions = []\n",
    "start = \"Mr. Brown,\"\n",
    "\n",
    "\"\"\"\n",
    "Notes:\n",
    "Temperature: trade-off between variety and politeness clarity\n",
    "Beam Search vs Top-k/Top-p. Beam search is a lot more coherent with a trade off for variety.\n",
    "\n",
    "Options:\n",
    "\n",
    "Naive Beam Search: Num_beams = 10, all else off\n",
    "Top K with Nucleus Sampling: top_p = 0.95, top_k = 10-20, do_sample=True\n",
    "Beam-search multinomial sampling : Num_beams = 10 + do_sample = True\n",
    "Diverse beam-search decoding: Num_beams = 10 + num_beam_groups = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "start_with_bos = '<|endoftext|>' + start\n",
    "encoded_prompt = trainer.tokenizer(start_with_bos, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "encoded_prompt = encoded_prompt.to(trainer.model.device)\n",
    "\n",
    "# prediction\n",
    "output_sequences = trainer.model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    max_length=160, #originally 160\n",
    "    min_length=10, #originally 10\n",
    "    temperature= 1, #originally 1\n",
    "    no_repeat_ngram_size=2,    \n",
    "    \n",
    "    \n",
    "    #Edit stuff here down to change decoding \n",
    "# BEAM\n",
    "#     num_beams=10, #on or off\n",
    "\n",
    "# TOP    \n",
    "#     do_sample=True, # for multinomial beam search and top sampling\n",
    "#     top_p = 0.95, #0.95\n",
    "#     top_k = 50, #10-20\n",
    "\n",
    "# MULTINOMIAL\n",
    "#     num_beams=10, #on or off\n",
    "#     do_sample=True, # for multinomial beam search and top sampling\n",
    "\n",
    "# DIVERSE \n",
    "    num_beams=10, #on or off  \n",
    "    num_beam_groups = 2, # on or off, must be a multiple of num_beams\n",
    "\n",
    "    \n",
    "    \n",
    "    num_return_sequences= 10, #must = num_beam for diverse beam-search\n",
    "    \n",
    "    )\n",
    "generated_sequences = []\n",
    "\n",
    "# decode prediction\n",
    "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    text = trainer.tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=True)\n",
    "    if not ALLOW_NEW_LINES:\n",
    "        limit = text.find('\\n')\n",
    "        text = text[: limit if limit != -1 else None]\n",
    "    generated_sequences.append(text.strip())\n",
    "\n",
    "for i, g in enumerate(generated_sequences):\n",
    "    predictions.append([start, g])\n",
    "\n",
    "for pair in predictions:\n",
    "  print(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning\n"
     ]
    }
   ],
   "source": [
    "print(\"warning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Target      Prompt  \\\n",
      "0  paddingtonbear  Mr. Brown,   \n",
      "1  paddingtonbear  Mr. Brown,   \n",
      "2  paddingtonbear  Mr. Brown,   \n",
      "3  paddingtonbear  Mr. Brown,   \n",
      "4  paddingtonbear  Mr. Brown,   \n",
      "5  paddingtonbear  Mr. Brown,   \n",
      "6  paddingtonbear  Mr. Brown,   \n",
      "7  paddingtonbear  Mr. Brown,   \n",
      "8  paddingtonbear  Mr. Brown,   \n",
      "9  paddingtonbear  Mr. Brown,   \n",
      "\n",
      "                                              Tweets     Type  \n",
      "0  Mr. Brown, I'm very pleased that you would lik...  neutral  \n",
      "1  Mr. Brown, I'm very pleased that you would lik...  neutral  \n",
      "2  Mr. Brown, I'm very pleased that you would lik...  neutral  \n",
      "3  Mr. Brown, I'm very pleased that you would lik...  neutral  \n",
      "4  Mr. Brown, I'm very pleased that you would lik...  neutral  \n",
      "5  Mr. Brown, I’m very pleased that you would lik...  neutral  \n",
      "6  Mr. Brown, I’m very pleased that you would lik...  neutral  \n",
      "7  Mr. Brown, I'm very pleased that you would lik...  neutral  \n",
      "8  Mr. Brown, I'm very pleased that you would lik...  neutral  \n",
      "9  Mr. Brown, I'm very pleased that you would lik...  neutral  \n"
     ]
    }
   ],
   "source": [
    "username = \"paddingtonbear\" #twitter user\n",
    "type_tweet = \"neutral\" #polite, impolite, neutral\n",
    "decoder = \"diverse\" #beam, top (top_k + top_p), multinomial (Num_beams + do_sample), diverse (Num_beams + num_beam_groups)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(columns = [\"Target\",\"Prompt\",\"Tweets\",\"Type\"])\n",
    "target_col = [username]*len(predictions) # target col\n",
    "type_col = [type_tweet]*len(predictions) # type col\n",
    "prompt_col = []\n",
    "tweets_col = []\n",
    "\n",
    "for pair in predictions:\n",
    "    prompt_col.append(pair[0])\n",
    "    tweets_col.append(pair[1])\n",
    "\n",
    "df[\"Target\"] = target_col\n",
    "df[\"Prompt\"] = prompt_col\n",
    "df[\"Tweets\"] = tweets_col\n",
    "df[\"Type\"] = type_col\n",
    "df.reset_index()\n",
    "# print(df)\n",
    "\n",
    "df.to_csv('responses/{}_{}_{}.csv'.format(username,type_tweet,decoder), index=False)\n",
    "#check writing:\n",
    "\n",
    "df_test = pd.read_csv('responses/{}_{}_{}.csv'.format(username,type_tweet,decoder))\n",
    "print(df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Unnamed: 0, Target, Tweets, Type]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBQuSNhMTpAT"
   },
   "source": [
    "# New Section\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "08e595c52ca3b9470036b1110e67b559e55f367cabc363f2e28d35631ed95060"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02e007fb503c4abea13fcdab99ba672c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cfb9c954923747d586a85fc1cf1442dc",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_98e279404402473783c5dafc59e78b58",
      "value": 0.002181832796966104
     }
    },
    "04f58430b59b4df8b5352bda202b47f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "753de470647f4d58ab2454117f2610b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f2e1d44a4bd491f91db7a44a5fa0938",
       "IPY_MODEL_02e007fb503c4abea13fcdab99ba672c"
      ],
      "layout": "IPY_MODEL_9c1cdb3f8f504888ae1947271af1061f"
     }
    },
    "7fe9e859d70e4399bf8e61f1382ccdd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f2e1d44a4bd491f91db7a44a5fa0938": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04f58430b59b4df8b5352bda202b47f9",
      "placeholder": "​",
      "style": "IPY_MODEL_7fe9e859d70e4399bf8e61f1382ccdd3",
      "value": "0.001 MB of 0.332 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "98e279404402473783c5dafc59e78b58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9c1cdb3f8f504888ae1947271af1061f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfb9c954923747d586a85fc1cf1442dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
