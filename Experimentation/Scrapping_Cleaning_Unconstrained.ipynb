{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5856e349",
   "metadata": {},
   "source": [
    "# Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e798fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d521f40",
   "metadata": {},
   "source": [
    "# 1. Data Gathering Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98f01117",
   "metadata": {},
   "source": [
    "### METHOD 1\n",
    "We first need to aquire our data. We choose the name of the account as well as the number of most recent tweets to aquire and then make the request with snscrape. The raw is then saved as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdcd817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables to be used in format string command below\n",
    "tweet_count = 10300\n",
    "username = \"elonmusk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20572b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note: runtime is around 5 minutes\n",
    "# Using OS library to call CLI commands in Python\n",
    "os.system(\"snscrape --jsonl --max-results {} twitter-search 'from:{}'> user-tweets.json\".format(tweet_count, username))\n",
    "# Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "tweets_df1 = pd.read_json('user-tweets.json', lines=True)\n",
    "\n",
    "# Displays first 5 entries from dataframe\n",
    "tweets_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03f33a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save raw\n",
    "file_name = \"data/individual_sets/tweets_{user}_{count}_m1\".format(user = username, count = tweet_count)\n",
    "tweets_df1.to_csv(file_name+\".csv\", sep=',', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71353ae0",
   "metadata": {},
   "source": [
    "### Method 2\n",
    "This method was test to compare an alterantive scrapping method but was discontinued later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0daf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pip install command below if you don't already have the library\n",
    "# !pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
    "\n",
    "# Run the below command if you don't already have Pandas\n",
    "# !pip install pandas\n",
    "\n",
    "# Imports\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a782b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe from the tweets list above\n",
    "tweets_df1 = pd.DataFrame(tweets_list1, columns=['Datetime', 'Tweet Id', 'Text', 'Username'])\n",
    "tweets_df2 = pd.DataFrame(tweets_list1)\n",
    "# Display first 5 entries from dataframe\n",
    "tweets_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245a8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe into a CSV\n",
    "file_name = \"data/\"+\"tweets_{user}_{count}_m1\".format(user = username, count = maxTweets)\n",
    "tweets_df2.to_csv('user-tweets2.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46d994",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df1.iloc[:, 2]\n",
    "# raw_html = tweets_df1.iloc[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ea9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean text experiment:\n",
    "\n",
    "print(\"original\\n\", tweets_df1.iloc[0, 2])\n",
    "\n",
    "clean_text = BeautifulSoup( tweets_df1.iloc[0, 2], \"lxml\").get_text(strip=True)\n",
    "print(\"filtered\\n\",clean_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b884f401",
   "metadata": {},
   "source": [
    "# 2. Cleaning dataset\n",
    "1. Remove all retweets\n",
    "Filter text\n",
    "1. Remove all url and picture links\n",
    "2. Remove short tweets\n",
    "3. Remove extra spaces \n",
    "4. Special character encoding\n",
    "\n",
    "Save File \n",
    "1. Save to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a17d7c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Temp\\ipykernel_17672\\1980682677.py:11: DtypeWarning: Columns (16,17,18,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_name)\n",
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#import API\n",
    "from api.filter import filter_manual, gen_input\n",
    "import pandas as pd\n",
    "#Read in data \n",
    "# file_name = file_name+\".csv\" #Insert file here\n",
    "file_name = 'data/individual_sets/tweets_elonmusk_20000_m1.csv'\n",
    "lang_choice = 'en'\n",
    "length_min = 4 #minimum tweet length\n",
    "\n",
    "\n",
    "data = pd.read_csv(file_name)\n",
    "df = data[['date','renderedContent', 'lang', 'sourceLabel', 'outlinks','media','retweetedTweet','quotedTweet','inReplyToUser','place']].copy()\n",
    "\n",
    "#Filter then Save as csv and txt\n",
    "data_new = filter_manual(df, length_min, lang_choice)\n",
    "data_new_lst = []\n",
    "data_new_lst.append(data_new)\n",
    "epochs = 4 #Shuffle parameter, see gen_input\n",
    "data_save = gen_input(data_new_lst,epochs)\n",
    "#TXT:\n",
    "with open(\"data/individual_sets/\"+\"cleaned_{user}_{count}\".format(user = username, count = tweet_count)+'.txt', 'w',encoding='utf-8') as f:\n",
    "    # for tweet in data_new:\n",
    "    #     f.write(tweet)\n",
    "    #     # print(tweet,\"AHHH\")\n",
    "    #     f.write('\\n')\n",
    "    f.write(data_save)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "\n",
    "df_clean = pd.DataFrame(data_new,columns=[\"Tweets\"])\n",
    "file_name = \"data/\"+\"cleaned_{user}_{count}_m1\".format(user = username, count = tweet_count)\n",
    "df_clean.to_csv(file_name+\".csv\", sep=',', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cc641a1",
   "metadata": {},
   "source": [
    "# 3.1 Politeness filtering: Unconstrained Model\n",
    "Here, we can play with politeness filtering by reading in specific files and getting the top x% of impolite or polite texts. This also lets us create specific subsets of politeness data that is then used in the unconstrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936acb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import API\n",
    "from api.filter import filter_manual, gen_input, gen_input_special_tokens\n",
    "from api.politeness import generate_politeness\n",
    "import pandas as pd\n",
    "#Stuff to run for first time:\n",
    "# python -m spacy download en_core_web_sm\n",
    "# pipip uninstall emoji\n",
    "# pip install emoji==1.7\n",
    "\n",
    "#Read in data \n",
    "############################################################\n",
    "#PARAMETERS:\n",
    "username = 'paddingtonbear'\n",
    "tweet_count = 103000\n",
    "file_name = 'data/individual_sets/cleaned_paddingtonbear_10300_m1.csv'\n",
    "lang_choice = 'en'\n",
    "length_min = 4 #minimum tweet length, unused here but good to have in mind\n",
    "#politeness params\n",
    "corpus_train = 'wikipedia'\n",
    "percentage_top_tweets = 0.5\n",
    "polite_or_impolite = 'impolite'\n",
    "############################################################\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(file_name)\n",
    "df_politeness = generate_politeness(df, corpus_train, percentage_top_tweets,polite_or_impolite) #choose by politeness\n",
    "print(\"number of data points\",len(df_politeness))\n",
    "data_new = df_politeness[\"Tweets\"]\n",
    "data_new_lst = []\n",
    "data_new_lst.append(data_new)\n",
    "epochs = 4 #Shuffle parameter, see gen_input\n",
    "\n",
    "data_save = gen_input(data_new_lst,epochs)\n",
    "with open(\"data/individual_sets/\"+\"{polite}_{user}_{count}_{perc}_finalnum{total}\".format(polite = polite_or_impolite, user = username, count = tweet_count,perc=percentage_top_tweets,total=len(df_politeness))+'.txt', 'w',encoding='utf-8') as f:\n",
    "    # for tweet in data_new:\n",
    "    #     f.write(tweet)\n",
    "    #     # print(tweet,\"AHHH\")\n",
    "    #     f.write('\\n')\n",
    "    f.write(data_save)\n",
    "\n",
    "    f.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fd8b06a",
   "metadata": {},
   "source": [
    "We now have all the data for the uncontrained models. The next steps are to generate the data for the uncontrained models which will take the cleaned data we generated in Section 2. This will be done in the file \"Full_Model_Processing_Constrained.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1c9efb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "08e595c52ca3b9470036b1110e67b559e55f367cabc363f2e28d35631ed95060"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
